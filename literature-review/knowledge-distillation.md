# Knowledge Distillation

## Definition
Methods that distill knowledge from larger deep neural netowkr model(s) to smaller model.

## Concepts
- Teacher-student architecture
- Knowledge
- Logits
- Dark knowledge
- Soft targets
- Hard targets
- Temperature
- Distillation loss
- Student-loss
- Distillation
    - Offline distillation
    - Online distillation
    - Self-distillation

## Reference
```bibtex
@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}
```